强化学习从零到RLHF（八）一图拆解RLHF中的PPO
https://zhuanlan.zhihu.com/p/635757674

[细(戏)说]RLHF场景下的PPO算法的来龙去脉
https://www.zhihu.com/tardis/bd/art/631338315?source_id=1001

六、GAE 广义优势估计
https://zhuanlan.zhihu.com/p/549145459

强化学习中值函数与优势函数的估计方法
https://zhuanlan.zhihu.com/p/345687962

HuggingFace 新作：70亿打败700亿Llama 2,开源模型Zephyr-7B，《 Direct Distillation of LM Alignment》！MAC可跑
https://zhuanlan.zhihu.com/p/663978474

DPO(Direct Preference Optimization):LLM的直接偏好优化
https://zhuanlan.zhihu.com/p/636122434
DPO——RLHF 的替代之《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》论文阅读
https://zhuanlan.zhihu.com/p/634705904
Direct Preference Optimization: Your Language Model is Secretly a Reward Model
https://arxiv.org/abs/2305.18290


DPO: Direct Preference Optimization 论文解读及代码实践
https://zhuanlan.zhihu.com/p/642569664

【LLM】Zephyr：直接提取语言模型的对齐(Zephyr: Direct Distillation of LM Alignment)
https://zhuanlan.zhihu.com/p/665111490
实战｜如何低成本训练一个可以超越 70B Llama2 的模型 Zephyr-7B
https://zhuanlan.zhihu.com/p/663782256
Zephyr: Direct Distillation of LM Alignment
https://arxiv.org/abs/2310.16944

ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？
https://www.zhihu.com/question/596230048

ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？ - 郁博文的回答 - 知乎
https://www.zhihu.com/question/596230048/answer/3102096962


逆强化学习：从专家策略中学习奖励函数的无监督方法
https://zhuanlan.zhihu.com/p/618067860?utm_id=0

模仿学习：逆向强化学习(Inverse Reinforcement Learning， IRL)
https://blog.csdn.net/qq_40206371/article/details/125063160

RLHF总结
https://www.cnblogs.com/end/p/17805338.html

人类偏好优化算法哪家强？跟着高手一文学懂DPO、IPO和KTO
https://zhuanlan.zhihu.com/p/682551974

SPIN：一个“不是DPO，胜似DPO”的方法
https://zhuanlan.zhihu.com/p/677732226
【LLM】自我对弈微调将弱语言模型转换为强语言模型
https://zhuanlan.zhihu.com/p/676951756
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
https://arxiv.org/abs/2401.01335

