强化学习从零到RLHF（八）一图拆解RLHF中的PPO
https://zhuanlan.zhihu.com/p/635757674

[细(戏)说]RLHF场景下的PPO算法的来龙去脉
https://www.zhihu.com/tardis/bd/art/631338315?source_id=1001

六、GAE 广义优势估计
https://zhuanlan.zhihu.com/p/549145459

强化学习中值函数与优势函数的估计方法
https://zhuanlan.zhihu.com/p/345687962

HuggingFace 新作：70亿打败700亿Llama 2,开源模型Zephyr-7B，《 Direct Distillation of LM Alignment》！MAC可跑
https://zhuanlan.zhihu.com/p/663978474

DPO(Direct Preference Optimization):LLM的直接偏好优化
https://zhuanlan.zhihu.com/p/636122434
DPO——RLHF 的替代之《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》论文阅读
https://zhuanlan.zhihu.com/p/634705904

DPO: Direct Preference Optimization 论文解读及代码实践
https://zhuanlan.zhihu.com/p/642569664

【LLM】Zephyr：直接提取语言模型的对齐(Zephyr: Direct Distillation of LM Alignment)
https://zhuanlan.zhihu.com/p/665111490

