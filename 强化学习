强化学习从零到RLHF（八）一图拆解RLHF中的PPO
https://zhuanlan.zhihu.com/p/635757674

[细(戏)说]RLHF场景下的PPO算法的来龙去脉
https://www.zhihu.com/tardis/bd/art/631338315?source_id=1001

六、GAE 广义优势估计
https://zhuanlan.zhihu.com/p/549145459

强化学习中值函数与优势函数的估计方法
https://zhuanlan.zhihu.com/p/345687962

HuggingFace 新作：70亿打败700亿Llama 2,开源模型Zephyr-7B，《 Direct Distillation of LM Alignment》！MAC可跑
https://zhuanlan.zhihu.com/p/663978474

DPO(Direct Preference Optimization):LLM的直接偏好优化
https://zhuanlan.zhihu.com/p/636122434
DPO——RLHF 的替代之《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》论文阅读
https://zhuanlan.zhihu.com/p/634705904

DPO: Direct Preference Optimization 论文解读及代码实践
https://zhuanlan.zhihu.com/p/642569664

【LLM】Zephyr：直接提取语言模型的对齐(Zephyr: Direct Distillation of LM Alignment)
https://zhuanlan.zhihu.com/p/665111490
实战｜如何低成本训练一个可以超越 70B Llama2 的模型 Zephyr-7B
https://zhuanlan.zhihu.com/p/663782256

ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？
https://www.zhihu.com/question/596230048

逆强化学习：从专家策略中学习奖励函数的无监督方法
https://zhuanlan.zhihu.com/p/618067860?utm_id=0

