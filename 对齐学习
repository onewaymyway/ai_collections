强化学习从零到RLHF（八）一图拆解RLHF中的PPO
https://zhuanlan.zhihu.com/p/635757674

[细(戏)说]RLHF场景下的PPO算法的来龙去脉
https://www.zhihu.com/tardis/bd/art/631338315?source_id=1001

ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？
https://www.zhihu.com/question/596230048

ChatGPT 为什么不用 Reward-Model 的数据直接 fine-tune，而用 RL？ - 郁博文的回答 - 知乎
https://www.zhihu.com/question/596230048/answer/3102096962


逆强化学习：从专家策略中学习奖励函数的无监督方法
https://zhuanlan.zhihu.com/p/618067860?utm_id=0

模仿学习：逆向强化学习(Inverse Reinforcement Learning， IRL)
https://blog.csdn.net/qq_40206371/article/details/125063160

RLHF总结
https://www.cnblogs.com/end/p/17805338.html

六、GAE 广义优势估计
https://zhuanlan.zhihu.com/p/549145459

强化学习中值函数与优势函数的估计方法
https://zhuanlan.zhihu.com/p/345687962

HuggingFace 新作：70亿打败700亿Llama 2,开源模型Zephyr-7B，《 Direct Distillation of LM Alignment》！MAC可跑
https://zhuanlan.zhihu.com/p/663978474

DPO(Direct Preference Optimization):LLM的直接偏好优化
https://zhuanlan.zhihu.com/p/636122434
DPO——RLHF 的替代之《Direct Preference Optimization: Your Language Model is Secretly a Reward Model》论文阅读
https://zhuanlan.zhihu.com/p/634705904
Direct Preference Optimization: Your Language Model is Secretly a Reward Model
https://arxiv.org/abs/2305.18290


DPO: Direct Preference Optimization 论文解读及代码实践
https://zhuanlan.zhihu.com/p/642569664

[Paper Reading] 深入对比 DPO 和 RLHF
https://zhuanlan.zhihu.com/p/676895603

【LLM】Zephyr：直接提取语言模型的对齐(Zephyr: Direct Distillation of LM Alignment)
https://zhuanlan.zhihu.com/p/665111490
实战｜如何低成本训练一个可以超越 70B Llama2 的模型 Zephyr-7B
https://zhuanlan.zhihu.com/p/663782256
Zephyr: Direct Distillation of LM Alignment
https://arxiv.org/abs/2310.16944

DPO as Verifier
https://zhuanlan.zhihu.com/p/682365020


人类偏好优化算法哪家强？跟着高手一文学懂DPO、IPO和KTO
https://zhuanlan.zhihu.com/p/682551974

SPIN：一个“不是DPO，胜似DPO”的方法
https://zhuanlan.zhihu.com/p/677732226
【LLM】自我对弈微调将弱语言模型转换为强语言模型
https://zhuanlan.zhihu.com/p/676951756
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
https://arxiv.org/abs/2401.01335

MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization
https://arxiv.org/abs/2401.06838

自学推理器的新突破！微软·谷歌联合研发V-STaR：为自学推理器训练验证器
https://zhuanlan.zhihu.com/p/682592746
V-STaR: Training Verifiers for Self-Taught Reasoners
https://arxiv.org/abs/2402.06457

LLaMA2自我进化超越GPT-4？Self-Rewarding Language Models 论文解读
https://zhuanlan.zhihu.com/p/679449495
Self-Rewarding Language Models 总结版
https://zhuanlan.zhihu.com/p/679297230
Self-Rewarding Language Models
https://arxiv.org/abs/2401.10020


解密prompt24. RLHF新方案之训练策略：SLiC-HF & DPO & RRHF & RSO
https://cloud.tencent.com/developer/article/2389619

只用500字说清楚什么是Bradley-Terry模型
https://zhuanlan.zhihu.com/p/679501567
ELO算法的原理及应用
https://zhuanlan.zhihu.com/p/57480433
【RLxLLM 基础】Preference Learning: Bradley–Terry, The Goodhart's Law, RLHF, ESPD与科研BoN策略(x
https://zhuanlan.zhihu.com/p/663790188

Noise Contrastive Alignment of Language Models with Explicit Rewards
https://arxiv.org/abs/2402.05369
Noise Contrastive Estimation 前世今生——从 NCE 到 InfoNCE
https://zhuanlan.zhihu.com/p/334772391

KTO: Model Alignment as Prospect Theoretic Optimization
https://arxiv.org/abs/2402.01306
https://github.com/ContextualAI/HALOs
前景理论的具体内容是什么？有哪些具体的应用？
https://www.zhihu.com/question/33525083

A General Theoretical Paradigm to Understand Learning from Human Preferences
https://arxiv.org/abs/2310.12036

Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
https://arxiv.org/abs/2402.10958
https://github.com/yinyueqin/relative-preference-optimization

对抗式大模型对齐：无需人类反馈也能RLHF
https://zhuanlan.zhihu.com/p/670194740
Adversarial Preference Optimization
https://arxiv.org/abs/2311.08045
https://github.com/linear95/apo

[Anthropic] Constitutional AI: 从RLHF到RLAIF，用AI训练AI！
https://zhuanlan.zhihu.com/p/612873436
Constitutional AI: Harmlessness from AI Feedback
https://arxiv.org/abs/2212.08073

IBM也下场LLM了，自对齐、高效率的单峰驼Dromedary来了
https://zhuanlan.zhihu.com/p/627643379
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision
https://zhuanlan.zhihu.com/p/628431087
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision
https://arxiv.org/abs/2305.03047

RLAIF细节分享&个人想法
https://zhuanlan.zhihu.com/p/657436655
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback
https://arxiv.org/abs/2309.00267

CITING: Large Language Models Create Curriculum for Instruction Tuning
https://arxiv.org/abs/2310.02527

A Critical Evaluation of AI Feedback for Aligning Large Language Models
https://arxiv.org/abs/2402.12366

[大语言模型之RRHF]RRHF: Rank Responses to Align Language Models with Human Feedback without tears（2023）
https://zhuanlan.zhihu.com/p/622198781
RRHF: Rank Responses to Align Language Models with Human Feedback without tears
https://arxiv.org/abs/2304.05302


一些RLHF的平替汇总（2023.11）
https://zhuanlan.zhihu.com/p/667152180

用 Decision Transformer/Offline RL 做 LLM Alignment
https://zhuanlan.zhihu.com/p/652335046
Aligning Language Models with Offline Learning from Human Feedback
https://arxiv.org/abs/2308.12050

NVIDIA 技术博客：宣布推出 SteerLM：在推理期间自定义 LLM 的简单实用技术
https://bbs.csdn.net/topics/617737059
SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF
https://arxiv.org/abs/2310.05344

【手撕RLHF-Aligner】7B模型外挂，暴涨GPT4安全性26.9%
https://zhuanlan.zhihu.com/p/682627363
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
https://arxiv.org/abs/2402.02416

Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment
https://arxiv.org/abs/2402.19085

West-of-N: Synthetic Preference Generation for Improved Reward Modeling
https://arxiv.org/abs/2401.12086

Aligning the Capabilities of Large Language Models with the Context of Information Retrieval via Contrastive Feedback
https://arxiv.org/abs/2309.17078

Constructive Large Language Models Alignment with Diverse Feedback
https://arxiv.org/abs/2310.06450


面壁智能对齐技术UltraFeedback如何让7B模型打败70B LLaMA2？
https://baijiahao.baidu.com/s?id=1787055223757619095
UltraFeedback: Boosting Language Models with High-quality Feedback
https://arxiv.org/abs/2310.01377
https://github.com/OpenBMB/UltraFeedback

Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation
https://arxiv.org/abs/2402.11907
