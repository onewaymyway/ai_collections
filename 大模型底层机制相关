A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks
https://arxiv.org/abs/2303.11873

谷歌发现大模型「领悟」现象！训练久了突然不再死记硬背，多么痛的领悟
https://zhuanlan.zhihu.com/p/649631256
Do Machine Learning Models Memorize or Generalize?
https://pair.withgoogle.com/explorables/grokking/


AI「领悟」有理论解释了！谷歌：两种脑回路内部竞争，训练久了突然不再死记硬背
https://zhuanlan.zhihu.com/p/656960508
Explaining grokking through circuit efficiency
https://arxiv.org/abs/2309.02390



Unifying Grokking and Double Descent
https://arxiv.org/abs/2303.06173

Progress measures for grokking via mechanistic interpretability
https://arxiv.org/abs/2301.05217


incontext learning：玄学->科学
https://zhuanlan.zhihu.com/p/620903768
In-context Learning and Induction Heads
https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html

Skill-it! A Data-Driven Skills Framework for Understanding and Training Language Models
https://arxiv.org/abs/2307.14430

解读 Anthropic 文章 Language Models (Mostly) Know What They Know（模型大多数情况下知道自己是否知道）
https://zhuanlan.zhihu.com/p/669033952
Language Models (Mostly) Know What They Know
https://arxiv.org/abs/2207.05221

不同数据集有不同的Scaling law？而你可用一个压缩算法来预测它
https://new.qq.com/rain/a/20240603A06PEN00
