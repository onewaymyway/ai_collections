博采众长！我全都要！Allen AI推出集成主流大语言模型的LLM-BLENDER框架
https://blog.csdn.net/qq_27590277/article/details/131148880

Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
https://arxiv.org/abs/2101.03961
NLP炼丹笔记：Switch Transformers 朴实无华 大招秒杀
https://zhuanlan.zhihu.com/p/344702054
Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
https://paperswithcode.com/paper/switch-transformers-scaling-to-trillion

稀疏性在机器学习中的发展趋势——Sparsity，稀疏激活，高效计算，MoE，稀疏注意力机制
https://zhuanlan.zhihu.com/p/463352552

【LLM 002】从Scaling Laws到MoE
https://zhuanlan.zhihu.com/p/636861410?utm_id=0

Brainformers: Trading Simplicity for Efficiency
https://arxiv.org/abs/2306.00008

Mixture-of-Experts with Expert Choice Routing
https://arxiv.org/abs/2202.09368

Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts
https://arxiv.org/abs/2305.14705

【随机 Expert】Taming Sparsely Activated Transformer with Stochastic Experts
https://zhuanlan.zhihu.com/p/522417453

初识MoE：OUTRAGEOUSLY LARGE NEURAL NETWORKS: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER
https://zhuanlan.zhihu.com/p/587221910

【LLM】Mixtral 8x7B的论文发布了
https://zhuanlan.zhihu.com/p/677701448
https://mistral.ai/news/mixtral-of-experts/
Mixtral of Experts
https://arxiv.org/abs/2401.04088
探索Mixtral 8x7B MOE，中文Finetune混合专家模型
https://zhuanlan.zhihu.com/p/674026711
Mixtral-8x7B MoE大模型微调实践，超越Llama2-65B
https://zhuanlan.zhihu.com/p/674028456
万字长文详解 Mixtral 8x7B - 价值20亿美元的 MoE 大语言模型
https://zhuanlan.zhihu.com/p/676114291
Mixtral-8x7B-Instruct-v0.1 使用记录
https://zhuanlan.zhihu.com/p/673109934
使用Mixtral-offloading在消费级硬件上运行Mixtral-8x7B
https://zhuanlan.zhihu.com/p/677591545
混合专家模型 (MoE) 详解
https://huggingface.co/blog/zh/moe
【手撕LLM-sMoE】离GPT4又近了一步
https://zhuanlan.zhihu.com/p/672726704
Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face
https://huggingface.co/blog/mixtral

黑客 George Hotz 爆料 GPT-4 由 8 个 MoE 模型组成，真的吗？
https://www.zhihu.com/question/607812079

https://www.deepspeed.ai/tutorials/mixture-of-experts-nlg/

Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints
https://arxiv.org/abs/2212.05055

Qwen1.5-MoE: 1/3的激活参数量达到7B模型的性能
https://qwenlm.github.io/zh/blog/qwen-moe/
https://github.com/QwenLM/Qwen1.5
https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat

【LLM-MoE】DeepSeekMoE：迈向混合专家语言模型的终极专业化
https://zhuanlan.zhihu.com/p/678111855

