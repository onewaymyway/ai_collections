如何评测一个大模型？（微软亚洲研究院 ）
https://zhuanlan.zhihu.com/p/656726594

LLM Evaluation 如何评估一个大模型？
https://zhuanlan.zhihu.com/p/644373658

关于评估大型语言模型的调查
https://zhuanlan.zhihu.com/p/643917053

放弃评测大模型，普林斯顿大学已经开始评估Prompt了，提出Prompt评估框架
https://zhuanlan.zhihu.com/p/644546392

大模型数据--评测数据汇总
https://zhuanlan.zhihu.com/p/658725797

国内大模型测评现状
https://zhuanlan.zhihu.com/p/660201722


用语言模型评估语言模型（3.1）Shepherd
https://zhuanlan.zhihu.com/p/665824328
Shepherd: A Critic for Language Model Generation
https://arxiv.org/abs/2308.04592
https://github.com/facebookresearch/Shepherd
"牧羊人"Shepherd中文版批判模型微调
https://zhuanlan.zhihu.com/p/665269693
https://huggingface.co/datasets/frankminors123/chinese-shepherd-critic-dataset

PandaLM: 评估大模型的大模型, 保护隐私、可靠、可复现，三行代码即可调用
https://zhuanlan.zhihu.com/p/626391857
PandaLM：面向LLM指令调优(论文翻译)
https://zhuanlan.zhihu.com/p/645760817
https://arxiv.org/abs/2306.05087

JudgeLM: Fine-tuned Large Language Models are Scalable Judges
https://arxiv.org/abs/2310.17631
https://github.com/baaivision/JudgeLM
https://huggingface.co/datasets/BAAI/JudgeLM-100K

Evaluating Large Language Models at Evaluating Instruction Following
https://arxiv.org/abs/2310.07641
陈丹琦新作：一个LLM的评估基准LLMBar
https://blog.csdn.net/qq_27590277/article/details/133874920
https://github.com/princeton-nlp/LLMBar

Generative Judge for Evaluating Alignment
https://arxiv.org/abs/2310.05470
评论能力强于GPT-4，上交开源13B评估大模型Auto-J
https://zhuanlan.zhihu.com/p/662639644
https://github.com/GAIR-NLP/auto-j

Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena
https://arxiv.org/abs/2306.05685

利用GPT4等进行大模型自动打分是否靠谱：3种评价方法、4大缺陷及4大应对方案工作解读
http://lechangxia.cc/gpt4/411.html

Large Language Models are not Fair Evaluators
https://arxiv.org/abs/2305.17926
用 GPT-4 评估模型，不一定靠谱
https://zhuanlan.zhihu.com/p/633632317

Can Large Language Models Understand Real-World Complex Instructions?
https://arxiv.org/abs/2309.09150
https://github.com/Abbey4799/CELLO

TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks
https://arxiv.org/abs/2310.00752

ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large Language Models
https://arxiv.org/abs/2308.14353

大模型是如何在评测中“作弊”的？
https://zhuanlan.zhihu.com/p/665426752

如何评价天工大模型技术报告中指出国内大模型用领域内数据刷榜的现象？
https://www.zhihu.com/question/628957425
Skywork: A More Open Bilingual Foundation Model
https://arxiv.org/abs/2310.19341

Split and Merge: Aligning Position Biases in Large Language Model based Evaluators
https://arxiv.org/abs/2310.01432

大模型的预训练数据还可以这么检测
https://zhuanlan.zhihu.com/p/665612302
Detecting Pretraining Data from Large Language Models
https://arxiv.org/abs/2310.16789

LONGEVAL：提升长篇摘要忠实度评估的方法（降低标准差从18.5至6.8）
https://zhuanlan.zhihu.com/p/665627544
https://arxiv.org/abs/2301.13298

如何优雅地自动评测 LLM 模型质量
https://zhuanlan.zhihu.com/p/666001842

Constructive Large Language Models Alignment with Diverse Feedback
https://arxiv.org/abs/2310.06450

谷歌提出TrueTeacher：基于大型语言模型的学习事实一致性评价
https://zhuanlan.zhihu.com/p/630842802
TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models
https://arxiv.org/abs/2305.11171

BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues
https://arxiv.org/abs/2310.13650

Skill-Mix: a Flexible and Expandable Family of Evaluations for AI models
https://arxiv.org/abs/2310.17567

CritiqueLLM：高质量、低成本的评分模型
https://zhuanlan.zhihu.com/p/671640706
CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation
https://arxiv.org/abs/2311.18702

T-Eval: Evaluating the Tool Utilization Capability Step by Step
https://arxiv.org/abs/2312.14033

大语言模型评估全解：评估流程、评估方法及常见问题
https://segmentfault.com/a/1190000044016949


【LLM/大模型】 Google：自我评估改进了大语言模型中的选择性生成
https://zhuanlan.zhihu.com/p/673156001
Self-Evaluation Improves Selective Generation in Large Language Models
https://arxiv.org/abs/2312.09300
